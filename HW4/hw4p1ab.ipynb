{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0e1766cebf3df6a1c3bfac07f2a721876b9859be16ef7357b0b9094e11afc09f4",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa450851bd0>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {6: 0, 7: 1, 8: 2, 9: 3}\n",
    "class_names = ['frog', 'horse', 'ship', 'truck']\n",
    "cifar4 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [6, 7, 8, 9]]\n",
    "cifar4_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.745326\n",
      "Epoch: 1, Loss: 0.646128\n",
      "Epoch: 2, Loss: 0.797680\n",
      "Epoch: 3, Loss: 0.797029\n",
      "Epoch: 4, Loss: 0.667015\n",
      "Epoch: 5, Loss: 0.583316\n",
      "Epoch: 6, Loss: 0.913675\n",
      "Epoch: 7, Loss: 0.500480\n",
      "Epoch: 8, Loss: 0.572505\n",
      "Epoch: 9, Loss: 0.684698\n",
      "Epoch: 10, Loss: 0.590175\n",
      "Epoch: 11, Loss: 0.713271\n",
      "Epoch: 12, Loss: 0.457858\n",
      "Epoch: 13, Loss: 0.451838\n",
      "Epoch: 14, Loss: 0.422193\n",
      "Epoch: 15, Loss: 0.513691\n",
      "Epoch: 16, Loss: 0.670527\n",
      "Epoch: 17, Loss: 0.325947\n",
      "Epoch: 18, Loss: 0.453996\n",
      "Epoch: 19, Loss: 0.557100\n",
      "Epoch: 20, Loss: 0.377720\n",
      "Epoch: 21, Loss: 0.351241\n",
      "Epoch: 22, Loss: 0.300203\n",
      "Epoch: 23, Loss: 0.329142\n",
      "Epoch: 24, Loss: 0.278761\n",
      "Epoch: 25, Loss: 0.240725\n",
      "Epoch: 26, Loss: 0.371935\n",
      "Epoch: 27, Loss: 0.171413\n",
      "Epoch: 28, Loss: 0.122777\n",
      "Epoch: 29, Loss: 0.242501\n",
      "Epoch: 30, Loss: 0.196924\n",
      "Epoch: 31, Loss: 0.106221\n",
      "Epoch: 32, Loss: 0.167923\n",
      "Epoch: 33, Loss: 0.076605\n",
      "Epoch: 34, Loss: 0.130320\n",
      "Epoch: 35, Loss: 0.127546\n",
      "Epoch: 36, Loss: 0.139069\n",
      "Epoch: 37, Loss: 0.173204\n",
      "Epoch: 38, Loss: 0.098087\n",
      "Epoch: 39, Loss: 0.104630\n",
      "Epoch: 40, Loss: 0.171795\n",
      "Epoch: 41, Loss: 0.083699\n",
      "Epoch: 42, Loss: 0.067718\n",
      "Epoch: 43, Loss: 0.038186\n",
      "Epoch: 44, Loss: 0.076516\n",
      "Epoch: 45, Loss: 0.066348\n",
      "Epoch: 46, Loss: 0.112702\n",
      "Epoch: 47, Loss: 0.093993\n",
      "Epoch: 48, Loss: 0.064829\n",
      "Epoch: 49, Loss: 0.042878\n",
      "Epoch: 50, Loss: 0.083492\n",
      "Epoch: 51, Loss: 0.064377\n",
      "Epoch: 52, Loss: 0.113614\n",
      "Epoch: 53, Loss: 0.109474\n",
      "Epoch: 54, Loss: 0.058128\n",
      "Epoch: 55, Loss: 0.022197\n",
      "Epoch: 56, Loss: 0.069295\n",
      "Epoch: 57, Loss: 0.037508\n",
      "Epoch: 58, Loss: 0.025592\n",
      "Epoch: 59, Loss: 0.018273\n",
      "Epoch: 60, Loss: 0.035010\n",
      "Epoch: 61, Loss: 0.016273\n",
      "Epoch: 62, Loss: 0.032881\n",
      "Epoch: 63, Loss: 0.051055\n",
      "Epoch: 64, Loss: 0.030540\n",
      "Epoch: 65, Loss: 0.022560\n",
      "Epoch: 66, Loss: 0.038194\n",
      "Epoch: 67, Loss: 0.036133\n",
      "Epoch: 68, Loss: 0.027442\n",
      "Epoch: 69, Loss: 0.015030\n",
      "Epoch: 70, Loss: 0.023017\n",
      "Epoch: 71, Loss: 0.023689\n",
      "Epoch: 72, Loss: 0.020755\n",
      "Epoch: 73, Loss: 0.012624\n",
      "Epoch: 74, Loss: 0.024573\n",
      "Epoch: 75, Loss: 0.026391\n",
      "Epoch: 76, Loss: 0.026680\n",
      "Epoch: 77, Loss: 0.023896\n",
      "Epoch: 78, Loss: 0.020158\n",
      "Epoch: 79, Loss: 0.030742\n",
      "Epoch: 80, Loss: 0.026650\n",
      "Epoch: 81, Loss: 0.013864\n",
      "Epoch: 82, Loss: 0.012385\n",
      "Epoch: 83, Loss: 0.010432\n",
      "Epoch: 84, Loss: 0.010260\n",
      "Epoch: 85, Loss: 0.014430\n",
      "Epoch: 86, Loss: 0.012687\n",
      "Epoch: 87, Loss: 0.010398\n",
      "Epoch: 88, Loss: 0.015221\n",
      "Epoch: 89, Loss: 0.011710\n",
      "Epoch: 90, Loss: 0.013729\n",
      "Epoch: 91, Loss: 0.012183\n",
      "Epoch: 92, Loss: 0.010537\n",
      "Epoch: 93, Loss: 0.009835\n",
      "Epoch: 94, Loss: 0.016884\n",
      "Epoch: 95, Loss: 0.007684\n",
      "Epoch: 96, Loss: 0.019139\n",
      "Epoch: 97, Loss: 0.011535\n",
      "Epoch: 98, Loss: 0.012063\n",
      "Epoch: 99, Loss: 0.051556\n",
      "Epoch: 100, Loss: 0.009063\n",
      "Epoch: 101, Loss: 0.014064\n",
      "Epoch: 102, Loss: 0.007180\n",
      "Epoch: 103, Loss: 0.013147\n",
      "Epoch: 104, Loss: 0.008137\n",
      "Epoch: 105, Loss: 0.011761\n",
      "Epoch: 106, Loss: 0.009348\n",
      "Epoch: 107, Loss: 0.013191\n",
      "Epoch: 108, Loss: 0.008609\n",
      "Epoch: 109, Loss: 0.010587\n",
      "Epoch: 110, Loss: 0.019434\n",
      "Epoch: 111, Loss: 0.014501\n",
      "Epoch: 112, Loss: 0.009024\n",
      "Epoch: 113, Loss: 0.006623\n",
      "Epoch: 114, Loss: 0.009394\n",
      "Epoch: 115, Loss: 0.012434\n",
      "Epoch: 116, Loss: 0.009425\n",
      "Epoch: 117, Loss: 0.005723\n",
      "Epoch: 118, Loss: 0.007306\n",
      "Epoch: 119, Loss: 0.012212\n",
      "Epoch: 120, Loss: 0.008513\n",
      "Epoch: 121, Loss: 0.009391\n",
      "Epoch: 122, Loss: 0.005982\n",
      "Epoch: 123, Loss: 0.007222\n",
      "Epoch: 124, Loss: 0.016035\n",
      "Epoch: 125, Loss: 0.006827\n",
      "Epoch: 126, Loss: 0.006042\n",
      "Epoch: 127, Loss: 0.004501\n",
      "Epoch: 128, Loss: 0.009183\n",
      "Epoch: 129, Loss: 0.008580\n",
      "Epoch: 130, Loss: 0.007872\n",
      "Epoch: 131, Loss: 0.007476\n",
      "Epoch: 132, Loss: 0.007418\n",
      "Epoch: 133, Loss: 0.007770\n",
      "Epoch: 134, Loss: 0.010097\n",
      "Epoch: 135, Loss: 0.008185\n",
      "Epoch: 136, Loss: 0.007580\n",
      "Epoch: 137, Loss: 0.007082\n",
      "Epoch: 138, Loss: 0.011108\n",
      "Epoch: 139, Loss: 0.007571\n",
      "Epoch: 140, Loss: 0.007281\n",
      "Epoch: 141, Loss: 0.007130\n",
      "Epoch: 142, Loss: 0.007788\n",
      "Epoch: 143, Loss: 0.007128\n",
      "Epoch: 144, Loss: 0.004938\n",
      "Epoch: 145, Loss: 0.007405\n",
      "Epoch: 146, Loss: 0.004450\n",
      "Epoch: 147, Loss: 0.006144\n",
      "Epoch: 148, Loss: 0.004300\n",
      "Epoch: 149, Loss: 0.005404\n",
      "Epoch: 150, Loss: 0.005417\n",
      "Epoch: 151, Loss: 0.005004\n",
      "Epoch: 152, Loss: 0.005829\n",
      "Epoch: 153, Loss: 0.006768\n",
      "Epoch: 154, Loss: 0.006653\n",
      "Epoch: 155, Loss: 0.004811\n",
      "Epoch: 156, Loss: 0.005572\n",
      "Epoch: 157, Loss: 0.004142\n",
      "Epoch: 158, Loss: 0.004304\n",
      "Epoch: 159, Loss: 0.004342\n",
      "Epoch: 160, Loss: 0.005187\n",
      "Epoch: 161, Loss: 0.006562\n",
      "Epoch: 162, Loss: 0.005223\n",
      "Epoch: 163, Loss: 0.007021\n",
      "Epoch: 164, Loss: 0.003506\n",
      "Epoch: 165, Loss: 0.005406\n",
      "Epoch: 166, Loss: 0.005753\n",
      "Epoch: 167, Loss: 0.004938\n",
      "Epoch: 168, Loss: 0.005410\n",
      "Epoch: 169, Loss: 0.003616\n",
      "Epoch: 170, Loss: 0.003201\n",
      "Epoch: 171, Loss: 0.005007\n",
      "Epoch: 172, Loss: 0.004252\n",
      "Epoch: 173, Loss: 0.002959\n",
      "Epoch: 174, Loss: 0.005649\n",
      "Epoch: 175, Loss: 0.004660\n",
      "Epoch: 176, Loss: 0.002589\n",
      "Epoch: 177, Loss: 0.004637\n",
      "Epoch: 178, Loss: 0.005632\n",
      "Epoch: 179, Loss: 0.005673\n",
      "Epoch: 180, Loss: 0.004769\n",
      "Epoch: 181, Loss: 0.002536\n",
      "Epoch: 182, Loss: 0.005473\n",
      "Epoch: 183, Loss: 0.004426\n",
      "Epoch: 184, Loss: 0.006090\n",
      "Epoch: 185, Loss: 0.002592\n",
      "Epoch: 186, Loss: 0.004454\n",
      "Epoch: 187, Loss: 0.003782\n",
      "Epoch: 188, Loss: 0.004063\n",
      "Epoch: 189, Loss: 0.003290\n",
      "Epoch: 190, Loss: 0.002372\n",
      "Epoch: 191, Loss: 0.004282\n",
      "Epoch: 192, Loss: 0.004809\n",
      "Epoch: 193, Loss: 0.002828\n",
      "Epoch: 194, Loss: 0.005009\n",
      "Epoch: 195, Loss: 0.005347\n",
      "Epoch: 196, Loss: 0.004429\n",
      "Epoch: 197, Loss: 0.003321\n",
      "Epoch: 198, Loss: 0.002365\n",
      "Epoch: 199, Loss: 0.007843\n",
      "329.6345603466034\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar4, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "time1 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "\n",
    "time2 = time.time()\n",
    "time3 = time2-time1\n",
    "print(time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "329.6345603466034\nAccuracy: 0.774000\n"
     ]
    }
   ],
   "source": [
    "print(time3)\n",
    "val_loader = torch.utils.data.DataLoader(cifar4_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.730969\n",
      "Epoch: 1, Loss: 0.850050\n",
      "Epoch: 2, Loss: 0.745921\n",
      "Epoch: 3, Loss: 0.643090\n",
      "Epoch: 4, Loss: 0.505526\n",
      "Epoch: 5, Loss: 0.720743\n",
      "Epoch: 6, Loss: 0.428833\n",
      "Epoch: 7, Loss: 0.489259\n",
      "Epoch: 8, Loss: 0.389046\n",
      "Epoch: 9, Loss: 0.352465\n",
      "Epoch: 10, Loss: 0.648342\n",
      "Epoch: 11, Loss: 0.727533\n",
      "Epoch: 12, Loss: 0.604521\n",
      "Epoch: 13, Loss: 0.421512\n",
      "Epoch: 14, Loss: 0.213547\n",
      "Epoch: 15, Loss: 0.613099\n",
      "Epoch: 16, Loss: 0.272522\n",
      "Epoch: 17, Loss: 0.458496\n",
      "Epoch: 18, Loss: 0.308636\n",
      "Epoch: 19, Loss: 0.575855\n",
      "Epoch: 20, Loss: 0.422337\n",
      "Epoch: 21, Loss: 0.427364\n",
      "Epoch: 22, Loss: 0.426739\n",
      "Epoch: 23, Loss: 0.269718\n",
      "Epoch: 24, Loss: 0.427971\n",
      "Epoch: 25, Loss: 0.084300\n",
      "Epoch: 26, Loss: 0.091484\n",
      "Epoch: 27, Loss: 0.049835\n",
      "Epoch: 28, Loss: 0.109861\n",
      "Epoch: 29, Loss: 0.118531\n",
      "Epoch: 30, Loss: 0.096298\n",
      "Epoch: 31, Loss: 0.235070\n",
      "Epoch: 32, Loss: 0.231155\n",
      "Epoch: 33, Loss: 0.069490\n",
      "Epoch: 34, Loss: 0.025683\n",
      "Epoch: 35, Loss: 0.013581\n",
      "Epoch: 36, Loss: 0.067123\n",
      "Epoch: 37, Loss: 0.026504\n",
      "Epoch: 38, Loss: 0.008804\n",
      "Epoch: 39, Loss: 0.006559\n",
      "Epoch: 40, Loss: 0.011367\n",
      "Epoch: 41, Loss: 0.006796\n",
      "Epoch: 42, Loss: 0.004786\n",
      "Epoch: 43, Loss: 0.005300\n",
      "Epoch: 44, Loss: 0.007755\n",
      "Epoch: 45, Loss: 0.001830\n",
      "Epoch: 46, Loss: 0.005296\n",
      "Epoch: 47, Loss: 0.003293\n",
      "Epoch: 48, Loss: 0.003100\n",
      "Epoch: 49, Loss: 0.002917\n",
      "Epoch: 50, Loss: 0.006050\n",
      "Epoch: 51, Loss: 0.003110\n",
      "Epoch: 52, Loss: 0.006715\n",
      "Epoch: 53, Loss: 0.001437\n",
      "Epoch: 54, Loss: 0.002865\n",
      "Epoch: 55, Loss: 0.001703\n",
      "Epoch: 56, Loss: 0.002627\n",
      "Epoch: 57, Loss: 0.001933\n",
      "Epoch: 58, Loss: 0.001694\n",
      "Epoch: 59, Loss: 0.002985\n",
      "Epoch: 60, Loss: 0.001406\n",
      "Epoch: 61, Loss: 0.002375\n",
      "Epoch: 62, Loss: 0.001342\n",
      "Epoch: 63, Loss: 0.001399\n",
      "Epoch: 64, Loss: 0.001563\n",
      "Epoch: 65, Loss: 0.000942\n",
      "Epoch: 66, Loss: 0.001863\n",
      "Epoch: 67, Loss: 0.002027\n",
      "Epoch: 68, Loss: 0.000850\n",
      "Epoch: 69, Loss: 0.001804\n",
      "Epoch: 70, Loss: 0.001736\n",
      "Epoch: 71, Loss: 0.001394\n",
      "Epoch: 72, Loss: 0.001747\n",
      "Epoch: 73, Loss: 0.001218\n",
      "Epoch: 74, Loss: 0.000781\n",
      "Epoch: 75, Loss: 0.000631\n",
      "Epoch: 76, Loss: 0.001199\n",
      "Epoch: 77, Loss: 0.001045\n",
      "Epoch: 78, Loss: 0.000841\n",
      "Epoch: 79, Loss: 0.000941\n",
      "Epoch: 80, Loss: 0.000965\n",
      "Epoch: 81, Loss: 0.000820\n",
      "Epoch: 82, Loss: 0.000891\n",
      "Epoch: 83, Loss: 0.000979\n",
      "Epoch: 84, Loss: 0.000356\n",
      "Epoch: 85, Loss: 0.001164\n",
      "Epoch: 86, Loss: 0.000822\n",
      "Epoch: 87, Loss: 0.000328\n",
      "Epoch: 88, Loss: 0.001137\n",
      "Epoch: 89, Loss: 0.000743\n",
      "Epoch: 90, Loss: 0.000506\n",
      "Epoch: 91, Loss: 0.000402\n",
      "Epoch: 92, Loss: 0.001019\n",
      "Epoch: 93, Loss: 0.000706\n",
      "Epoch: 94, Loss: 0.000708\n",
      "Epoch: 95, Loss: 0.001063\n",
      "Epoch: 96, Loss: 0.000461\n",
      "Epoch: 97, Loss: 0.000395\n",
      "Epoch: 98, Loss: 0.000753\n",
      "Epoch: 99, Loss: 0.000599\n",
      "Epoch: 100, Loss: 0.000567\n",
      "Epoch: 101, Loss: 0.000328\n",
      "Epoch: 102, Loss: 0.000456\n",
      "Epoch: 103, Loss: 0.000465\n",
      "Epoch: 104, Loss: 0.000991\n",
      "Epoch: 105, Loss: 0.000215\n",
      "Epoch: 106, Loss: 0.000539\n",
      "Epoch: 107, Loss: 0.000458\n",
      "Epoch: 108, Loss: 0.000458\n",
      "Epoch: 109, Loss: 0.001380\n",
      "Epoch: 110, Loss: 0.000529\n",
      "Epoch: 111, Loss: 0.000400\n",
      "Epoch: 112, Loss: 0.000605\n",
      "Epoch: 113, Loss: 0.000420\n",
      "Epoch: 114, Loss: 0.000214\n",
      "Epoch: 115, Loss: 0.000614\n",
      "Epoch: 116, Loss: 0.000451\n",
      "Epoch: 117, Loss: 0.001121\n",
      "Epoch: 118, Loss: 0.000573\n",
      "Epoch: 119, Loss: 0.000223\n",
      "Epoch: 120, Loss: 0.000700\n",
      "Epoch: 121, Loss: 0.000394\n",
      "Epoch: 122, Loss: 0.000361\n",
      "Epoch: 123, Loss: 0.000540\n",
      "Epoch: 124, Loss: 0.000264\n",
      "Epoch: 125, Loss: 0.000626\n",
      "Epoch: 126, Loss: 0.000441\n",
      "Epoch: 127, Loss: 0.000447\n",
      "Epoch: 128, Loss: 0.000467\n",
      "Epoch: 129, Loss: 0.000382\n",
      "Epoch: 130, Loss: 0.000350\n",
      "Epoch: 131, Loss: 0.000435\n",
      "Epoch: 132, Loss: 0.000360\n",
      "Epoch: 133, Loss: 0.000312\n",
      "Epoch: 134, Loss: 0.000662\n",
      "Epoch: 135, Loss: 0.000574\n",
      "Epoch: 136, Loss: 0.000315\n",
      "Epoch: 137, Loss: 0.000546\n",
      "Epoch: 138, Loss: 0.000543\n",
      "Epoch: 139, Loss: 0.000309\n",
      "Epoch: 140, Loss: 0.000403\n",
      "Epoch: 141, Loss: 0.000287\n",
      "Epoch: 142, Loss: 0.000343\n",
      "Epoch: 143, Loss: 0.000501\n",
      "Epoch: 144, Loss: 0.000444\n",
      "Epoch: 145, Loss: 0.000178\n",
      "Epoch: 146, Loss: 0.000407\n",
      "Epoch: 147, Loss: 0.000621\n",
      "Epoch: 148, Loss: 0.000444\n",
      "Epoch: 149, Loss: 0.000578\n",
      "Epoch: 150, Loss: 0.000488\n",
      "Epoch: 151, Loss: 0.000340\n",
      "Epoch: 152, Loss: 0.000233\n",
      "Epoch: 153, Loss: 0.000397\n",
      "Epoch: 154, Loss: 0.000205\n",
      "Epoch: 155, Loss: 0.000274\n",
      "Epoch: 156, Loss: 0.000217\n",
      "Epoch: 157, Loss: 0.000336\n",
      "Epoch: 158, Loss: 0.000509\n",
      "Epoch: 159, Loss: 0.000204\n",
      "Epoch: 160, Loss: 0.000415\n",
      "Epoch: 161, Loss: 0.000205\n",
      "Epoch: 162, Loss: 0.000423\n",
      "Epoch: 163, Loss: 0.000232\n",
      "Epoch: 164, Loss: 0.000300\n",
      "Epoch: 165, Loss: 0.000351\n",
      "Epoch: 166, Loss: 0.000125\n",
      "Epoch: 167, Loss: 0.000337\n",
      "Epoch: 168, Loss: 0.000275\n",
      "Epoch: 169, Loss: 0.000263\n",
      "Epoch: 170, Loss: 0.000324\n",
      "Epoch: 171, Loss: 0.000265\n",
      "Epoch: 172, Loss: 0.000238\n",
      "Epoch: 173, Loss: 0.000117\n",
      "Epoch: 174, Loss: 0.000244\n",
      "Epoch: 175, Loss: 0.000282\n",
      "Epoch: 176, Loss: 0.000374\n",
      "Epoch: 177, Loss: 0.000404\n",
      "Epoch: 178, Loss: 0.000305\n",
      "Epoch: 179, Loss: 0.000335\n",
      "Epoch: 180, Loss: 0.000276\n",
      "Epoch: 181, Loss: 0.000116\n",
      "Epoch: 182, Loss: 0.000377\n",
      "Epoch: 183, Loss: 0.000266\n",
      "Epoch: 184, Loss: 0.000229\n",
      "Epoch: 185, Loss: 0.000165\n",
      "Epoch: 186, Loss: 0.000206\n",
      "Epoch: 187, Loss: 0.000226\n",
      "Epoch: 188, Loss: 0.000319\n",
      "Epoch: 189, Loss: 0.000181\n",
      "Epoch: 190, Loss: 0.000334\n",
      "Epoch: 191, Loss: 0.000207\n",
      "Epoch: 192, Loss: 0.000634\n",
      "Epoch: 193, Loss: 0.000207\n",
      "Epoch: 194, Loss: 0.000234\n",
      "Epoch: 195, Loss: 0.000162\n",
      "Epoch: 196, Loss: 0.000267\n",
      "Epoch: 197, Loss: 0.000235\n",
      "Epoch: 198, Loss: 0.000230\n",
      "Epoch: 199, Loss: 0.000197\n",
      "855.9066932201385\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar4, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 4),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "time1 = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
    "\n",
    "time2 = time.time()\n",
    "time3 = time2-time1\n",
    "print(time3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "855.9066932201385\nAccuracy: 0.769250\n"
     ]
    }
   ],
   "source": [
    "print(time3)\n",
    "val_loader = torch.utils.data.DataLoader(cifar4_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}